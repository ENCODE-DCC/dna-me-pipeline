#!/usr/bin/env python
# coding: utf-8
# test-py-parallel 0.0.1
# Generated by dx-app-wizard.
#
# Scatter-process-gather execution pattern: Your app will split its
# input into multiple pieces, each of which will be processed in
# parallel, after which they are gathered together in some final
# output.
#
# This pattern is very similar to the "parallelized" template.  What
# it does differently is that it formally breaks out the "scatter"
# phase as a separate black-box entry point in the app.  (As a side
# effect, this requires a "map" entry point to call "process" on each
# of the results from the "scatter" phase.)
#
# Note that you can also replace any entry point in this execution
# pattern with an API call to run a separate app or applet.
#
# The following is a Unicode art picture of the flow of execution.
# Each box is an entry point, and vertical lines indicate that the
# entry point connected at the top of the line calls the entry point
# connected at the bottom of the line.  The letters represent the
# different stages in which the input is transformed, e.g. the output
# of the "scatter" entry point ("array:B") is given to the "map" entry
# point as input.  The "map" entry point calls as many "process" entry
# points as there are elements in its array input and gathers the
# results in its array output.
#
#          ┌──────┐
#       A->│ main │->D (output from "postprocess")
#          └┬─┬─┬─┘
#           │ │ │
#          ┌┴──────┐
#       A->│scatter│->array:B
#          └───────┘
#             │ │
#            ┌┴──────────────┐
#   array:B->│      map      │->array:C
#            └─────────┬─┬─┬─┘
#               │      │ . .
#               │     ┌┴──────┐
#               │  B->│process│->C
#               │     └───────┘
#            ┌──┴────────┐
#   array:C->│postprocess│->D
#            └───────────┘
#
# A = original app input, split up by "scatter" into pieces of type B
# B = an input that will be provided to a "process" entry point
# C = the output of a "process" entry point
# D = app output aggregated from the outputs of the "process" entry points
#
# See https://wiki.dnanexus.com/Developer-Portal for documentation and
# tutorials on how to modify this file.
#
# DNAnexus Python Bindings (dxpy) documentation:
#   http://autodoc.dnanexus.com/bindings/python/current/

import os
import dxpy
import subprocess
import shlex
import glob
#import logging

#logger = logging.getLogger(__name__)
#logger.addHandler(dxpy.DXLogHandler())
#logger.propagate = False

STRIP_EXTENSIONS = ['.gz', '.fq', '.fastq', '.fa', '.fasta']


def strip_extensions(filename, extensions):
    basename = filename
    for extension in extensions:
        basename = basename.rpartition(extension)[0] or basename
    return basename



@dxpy.entry_point("postprocess")
def postprocess(process_outputs, additional_input):
    # This is the "gather" phase which aggregates and performs any
    # additional computation after the "map" (and therefore after all
    # the "process") jobs are done.

    for item in process_outputs:
        print item

    return { "final_output": "postprocess placeholder output" }

@dxpy.entry_point("process")
def process(scattered_input, processed_input):
    # Fill in code here to process the input and create output.

    dme_ix = dxpy.DXFile(processed_input['dme_ix'])
    ncpus = processed_input['ncpus']
    reads_root = processed_input['reads_root']
    bam_root = reads_root + '_techrep'

    # The following line(s) download your file inputs to the local file system
    # using variable names for the filenames.

    dxpy.download_dxfile(dme_ix.get_id(), "index.tgz")
    dxpy.download_dxfile(scattered_input.get_id(), "split.fq")

    #logger.info("* === Calling DNAnexus and ENCODE independent script... ===")
    print("* === Calling DNAnexus and ENCODE independent script... ===")
    # subprocess.check_call('/usr/bin/dname_align_se.sh index.tgz %s %d %s' %
    # (reads_root, ncpus, bam_root))
    #logger.debug('EXAMPLE dname_align_se.sh index.tgz %s %d %s' % (reads_root, ncpus, bam_root))
    #logger.info("* === Returned from dnanexus post align ===")
    print('EXAMPLE dname_align_se.sh index.tgz %s %d %s' % (reads_root, ncpus, bam_root))
    print("* === Returned from dnanexus post align ===")



    # As always, you can choose not to return output if the
    # "postprocess" stage does not require any input, e.g. rows have
    # been added to a GTable that has been created in advance.  Just
    # make sure that the "postprocess" job does not run until all
    # "process" jobs have finished by making it wait for "map" to
    # finish using the depends_on argument (this is already done for
    # you in the invocation of the "postprocess" job in "main").

    return {"process_output": "process placeholder output"}


@dxpy.entry_point("map")
def map_entry_point(array_of_scattered_input, process_input):
    # The following calls "process" for each of the items in
    # *array_of_scattered_input*, using as input the item in the
    # array, as well as the rest of the fields in *process_input*.
    process_jobs = []
    for item in array_of_scattered_input:
        process_input["scattered_input"] = item
        process_jobs.append(dxpy.new_dxjob(fn_input=process_input, fn_name="process"))
    return { "process_outputs": [subjob.get_output_ref("process_output") for subjob in process_jobs] }


@dxpy.entry_point("scatter")
def scatter(scatter_input):
    # Fill in code here to do whatever is necessary to scatter the
    # input.
    array_of_scattered_input = []
    splitsize = scatter_input['split_size'] * 1000000
    subprocess.check_call('mkdir splits')
    for orig_file in scatter_input['orig_reads']:

        subprocess.check_call('/bin/zcat %s | /usr/bin/split -l %d -d - %s ' %
                             (orig_file, splitsize, 'splits/'+strip_extensions(orig_file)), shell=True)

    splits = os.listdir('splits')
    #logger.info("Return from scatter: %s" % splits)
    return {"array_of_scattered_input":
            [dxpy.dxlink(dxpy.upload_local_file(split_file)) for split_file in splits]}


def simplify_name():
    # Try to simplify the names

    rep_root = ''
    if os.path.isfile('/usr/bin/parse_property.py'):
        rep_root = subprocess.check_output(['parse_property.py', '--job', os.environ['DX_JOB_ID'],'--root_name', '--quiet'], shell=True)

    return rep_root


@dxpy.entry_point("main")
def main(reads, dme_ix, ncpus, splitsize):

    # The following line(s) initialize your data object inputs on the platform
    # into dxpy.DXDataObject instances that you can start using immediately.

    #dx_reads = [dxpy.DXFile(item) for item in reads]

    # The following line(s) download your file inputs to the local file system
    # using variable names for the filenames.


    read_files = []
    for i, f in enumerate(reads):
        reads_filename = dxpy.describe(f)['name']
        reads_basename = strip_extensions(reads_filename, STRIP_EXTENSIONS)
        fn = reads_basename + '_' + str(i) + 'fq.gz'
        dxpy.download_dxfile(dxpy.DXFile(f).get_id(), fn)
        read_files.append(fn)

    reads_root_name = simplify_name() or reads_basename

    # We first create the "scatter" job which will scatter some input
    # (replace with your own input as necessary).
    scatter_job = dxpy.new_dxjob(fn_input={
                                 'reads_name': reads_root_name,
                                 'orig_reads': read_files,
                                 'split_size': splitsize,
                                 },
                                 fn_name="scatter")

    # We will want to call "process" on each output of "scatter", so
    # we call the "map" entry point to do so.  We can also provide
    # here additional input that we want each "process" entry point to
    # receive, e.g. a GTable ID to which the "process" function should
    # add rows of data.
    map_input = {
        "array_of_scattered_input": scatter_job.get_output_ref("array_of_scattered_input"),
        "reads_root": reads_root_name,
        "process_input": {
            "ncpus": ncpus,
            "dme_ix": dme_ix
            }
        }
    map_job = dxpy.new_dxjob(fn_input=map_input, fn_name="map")

    # Finally, we want the "postprocess" job to run after "map" is
    # done calling "process" on each of its inputs.  Note that a job
    # is marked as "done" only after all of its child jobs are also
    # marked "done".
    postprocess_input = {
        "process_outputs": map_job.get_output_ref("process_outputs"),
        "additional_input": "gtable ID, for example"
        }
    postprocess_job = dxpy.new_dxjob(fn_input=postprocess_input,
                                     fn_name="postprocess",
                                     depends_on=[map_job])

    # The following line(s) use the Python bindings to upload your file outputs
    # after you have created them on the local file system.  It assumes that you
    # have used the output field name for the filename for each output, but you
    # can change that behavior to suit your needs.

    bam_techrep = dxpy.upload_local_file("bam_techrep")
    bam_techrep_qc = dxpy.upload_local_file("bam_techrep_qc")
    map_techrep = dxpy.upload_local_file("map_techrep")

    # If you would like to include any of the output fields from the
    # postprocess_job as the output of your app, you should return it
    # here using a job-based object reference.
    #
    # return { "app_output_field": postprocess_job.get_output_ref("final_output"), ...}
    #
    # Tip: you can include in your output at this point any open
    # objects (such as gtables) which will be closed by a job that
    # finishes later.  The system will check to make sure that the
    # output object is closed and will attempt to clone it out as
    # output into the parent container only after all subjobs have
    # finished.

    output = {}
    output["bam_techrep"] = dxpy.dxlink(bam_techrep)
    output["bam_techrep_qc"] = dxpy.dxlink(bam_techrep_qc)
    output["map_techrep"] = dxpy.dxlink(map_techrep)
    output["reads"] = reads
    output["metadata"] = postprocess_job.get_output_ref("metadata")

    return output

dxpy.run()
